# Machine-Running-Exercises

<h1>2019-03-23</h1>

- ( + machine_1.py ) 
<blockquote>
Output to scatter plot using iris_dataset
</blockquote>
<h1>2019-03-26</h1>

- ( + K-NN.py ) 
<blockquote>
First, the K-nearest neighbors classification, plt when n_neighbors = 1 and n_neighbors = 3.

I tried to express it as a figure.

The second is K-NN classifier.
</blockquote>
<hr>
- ( + n_neighbors_graph.py ) 
<blockquote>
Depending on the values of the test accuracy and the training accuracy, the over-fit, 

the optimal point, and the under-fit are graphically displayed.
</blockquote>

<h1>2019 - 03 -29</h1>

- ( + Dataset_wave.py )

- ( + Dataset_forge.py ) 

- ( + Dataset_cancer.py ) 

- ( + Dataset_Bostonc.py ) 

- ( + Dataset_Extended_boston.py ) 

- ( + GeneralizationVsComplexity_RelationshipsWithCancerDataset.py ) 

- ( + k-nearest-neighbors-return.py ) 


<blockquote>
i've tried a variety of datasets and a quick look at what they are.

Use CancerDataset to find the relationship between generalization and complexity.

k-Nearest Neighbor classification
</blockquote>
<h1>2019 - 04 - 08</h1>

- ( + Ridge.py ) 

- ( + Lasso.py ) 

- ( + Square root with Bostondataset.py )

- ( + Square root with wavedataset.py )

<blockquote>
I studied Ridge Regression and Lassie Recursion in Boston as an extension.

I studied the least square root with Boston data and Wave data.
</blockquote>
<h1>2019 - 04 -09</h1>

- ( + Weight.py ) 
<blockquote>
i studied of What is a weight?
</blockquote>
